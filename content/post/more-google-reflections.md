---
layout: post
title: istio, blink and you'll miss it
date: "2019-09-15"
---

Listen: I'm going to talk about Istio. I've put off writing this for too long, and I'm suddenly afraid that by the time I get around to it, I'll have forgotten everything I knew. Or, more likely, every feature I worked on will have been long-since deprecated (I read somewhere that the [Node Agent](https://istio.io/docs/concepts/security/#node-agent-in-kubernetes) would be deprecated, but I'm having trouble finding that changelist now).

Actually though, we can't always turn a blind eye to where our containers are running, one reason being hardware differences between our cluster's machines, so Kubernetes has to have things like [taints and tolerations](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/). We have to kludge on API features to reveal our own abstraction. This is like a magician *asking us* where he should hide his dove before doing a trick. Going deeper, containers let us pretend that our applications are cozied up in their own isolated environment while actually running atop the same operating system. An operating system lets us pretend that our computer is anything more than a few chips soldered together. And so it continues down the rabbit hole.

Istio provides abstractions that let us command our services like we're omnipotent beings, looking down on them from way, way above. We want to split traffic between an old and new version of a service, so we impose some YAML upon our cluster describing that desire. Miraculously, 5% of our production traffic redirects itself to the new service. Magic, right? Now we can perform incremental upgrades, mitigating the risk of change. Or better, if we use [traffic mirroring](https://istio.io/docs/tasks/traffic-management/mirroring/) to battle test our services, we can make sure it works before sacrificing _any_ production traffic over to it. Application-transparent distributed tracing (which Google has had for a [hot minute](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36356.pdf)), is awe. Encrypted service-to-service traffic, with no extra configuration, and automatic certificate issuance and rotation? Sign me up.

But of course, I'm taking you on a sunny stroll down the happy path. Istio comes at the cost of tremendous underlying complexity. That might be the biggest problem with Istio: the stunning surface area of the API it offers. Even within teams who spend 100% of their time on Istio it's hard to get a grip on it. It's arrogant of the project to assume that people running this service mesh in their cluster should have to understand ALL the underlying components. And you *really* might be up shit creek if you deploy Istio without a firm grasp of all the components it drops in your cluster, which is a *fuck of a lot*.
